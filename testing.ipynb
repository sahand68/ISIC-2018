{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import random\n",
    "from numpy.random import randint\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm as tqdm\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import torchvision.transforms as transforms\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import cv2\n",
    "import matplotlib\n",
    "import copy\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import glob\n",
    "mask_aug_ids = next(os.walk('C://Users//sahan//ipthw//Melanoma_segmentation//data//aug_mask//'))[2]\n",
    "image_aug_ids = next(os.walk('C://Users//sahan//ipthw//Melanoma_segmentation//data//aug_train//'))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms= {\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elastic_transform(image, alpha, sigma, pad_size=30, seed=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image\n",
    "        alpha : α is a scaling factor\n",
    "        sigma :  σ is an elasticity coefficient\n",
    "        random_state = random integer\n",
    "        Return :\n",
    "        image : elastically transformed numpy array of image\n",
    "    \"\"\"\n",
    "    image_size = int(image.shape[0])\n",
    "    image = np.pad(image, pad_size, mode=\"symmetric\")\n",
    "    if seed is None:\n",
    "        seed = randint(1, 100)\n",
    "        random_state = np.random.RandomState(seed)\n",
    "    else:\n",
    "        random_state = np.random.RandomState(seed)\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))\n",
    "    return cropping(map_coordinates(image, indices, order=1).reshape(shape), 512, pad_size, pad_size), seed\n",
    "\n",
    "\n",
    "def flip(image, option_value):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image\n",
    "        option_value = random integer between 0 to 3\n",
    "    Return :\n",
    "        image : numpy array of flipped image\n",
    "    \"\"\"\n",
    "    if option_value == 0:\n",
    "        # vertical\n",
    "        image = np.flip(image, option_value)\n",
    "    elif option_value == 1:\n",
    "        # horizontal\n",
    "        image = np.flip(image, option_value)\n",
    "    elif option_value == 2:\n",
    "        # horizontally and vertically flip\n",
    "        image = np.flip(image, 0)\n",
    "        image = np.flip(image, 1)\n",
    "    else:\n",
    "        image = image\n",
    "        # no effect\n",
    "    return image\n",
    "\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, std=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image\n",
    "        mean : pixel mean of image\n",
    "        standard deviation : pixel standard deviation of image\n",
    "    Return :\n",
    "        image : numpy array of image with gaussian noise added\n",
    "    \"\"\"\n",
    "    gaus_noise = np.random.normal(mean, std, image.shape)\n",
    "    image = image.astype(\"int16\")\n",
    "    noise_img = image + gaus_noise\n",
    "    image = ceil_floor_image(image)\n",
    "    return noise_img\n",
    "\n",
    "\n",
    "def add_uniform_noise(image, low=-10, high=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image\n",
    "        low : lower boundary of output interval\n",
    "        high : upper boundary of output interval\n",
    "    Return :\n",
    "        image : numpy array of image with uniform noise added\n",
    "    \"\"\"\n",
    "    uni_noise = np.random.uniform(low, high, image.shape)\n",
    "    image = image.astype(\"int16\")\n",
    "    noise_img = image + uni_noise\n",
    "    image = ceil_floor_image(image)\n",
    "    return noise_img\n",
    "\n",
    "\n",
    "def change_brightness(image, value):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image\n",
    "        value : brightness\n",
    "    Return :\n",
    "        image : numpy array of image with brightness added\n",
    "    \"\"\"\n",
    "    image = image.astype(\"int16\")\n",
    "    image = image + value\n",
    "    image = ceil_floor_image(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def ceil_floor_image(image):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image in datatype int16\n",
    "    Return :\n",
    "        image : numpy array of image in datatype uint8 with ceilling(maximum 255) and flooring(minimum 0)\n",
    "    \"\"\"\n",
    "    image[image > 255] = 255\n",
    "    image[image < 0] = 0\n",
    "    image = image.astype(\"uint8\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def approximate_image(image):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image : numpy array of image in datatype int16\n",
    "    Return :\n",
    "        image : numpy array of image in datatype uint8 only with 255 and 0\n",
    "    \"\"\"\n",
    "    image[image > 127.5] = 255\n",
    "    image[image < 127.5] = 0\n",
    "    image = image.astype(\"uint8\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalization1(image, mean, std):\n",
    "    \"\"\" Normalization using mean and std\n",
    "    Args :\n",
    "        image : numpy array of image\n",
    "        mean :\n",
    "    Return :\n",
    "        image : numpy array of image with values turned into standard scores\n",
    "    \"\"\"\n",
    "\n",
    "    image = image / 255  # values will lie between 0 and 1.\n",
    "    image = (image - mean) / std\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalization2(image, max, min):\n",
    "    \"\"\"Normalization to range of [min, max]\n",
    "    Args :\n",
    "        image : numpy array of image\n",
    "        mean :\n",
    "    Return :\n",
    "        image : numpy array of image with values turned into standard scores\n",
    "    \"\"\"\n",
    "    image_new = (image - np.min(image))*(max - min)/(np.max(image)-np.min(image)) + min\n",
    "    return image_new\n",
    "\n",
    "\n",
    "def stride_size(image_len, crop_num, crop_size):\n",
    "    \"\"\"return stride size\n",
    "    Args :\n",
    "        image_len(int) : length of one size of image (width or height)\n",
    "        crop_num(int) : number of crop in certain direction\n",
    "        crop_size(int) : size of crop\n",
    "    Return :\n",
    "        stride_size(int) : stride size\n",
    "    \"\"\"\n",
    "    return int((image_len - crop_size)/(crop_num - 1))\n",
    "\n",
    "\n",
    "def multi_cropping(image, crop_size, crop_num1, crop_num2):\n",
    "    \"\"\"crop the image and pad it to in_size\n",
    "    Args :\n",
    "        images : numpy arrays of images\n",
    "        crop_size(int) : size of cropped image\n",
    "        crop_num2 (int) : number of crop in horizontal way\n",
    "        crop_num1 (int) : number of crop in vertical way\n",
    "    Return :\n",
    "        cropped_imgs : numpy arrays of stacked images\n",
    "    \"\"\"\n",
    "\n",
    "    img_height, img_width = image.shape[0], image.shape[1]\n",
    "    assert crop_size*crop_num1 >= img_width and crop_size * \\\n",
    "        crop_num2 >= img_height, \"Whole image cannot be sufficiently expressed\"\n",
    "    assert crop_num1 <= img_width - crop_size + 1 and crop_num2 <= img_height - \\\n",
    "        crop_size + 1, \"Too many number of crops\"\n",
    "\n",
    "    cropped_imgs = []\n",
    "    # int((img_height - crop_size)/(crop_num1 - 1))\n",
    "    dim1_stride = stride_size(img_height, crop_num1, crop_size)\n",
    "    # int((img_width - crop_size)/(crop_num2 - 1))\n",
    "    dim2_stride = stride_size(img_width, crop_num2, crop_size)\n",
    "    for i in range(crop_num1):\n",
    "        for j in range(crop_num2):\n",
    "            cropped_imgs.append(cropping(image, crop_size,\n",
    "                                         dim1_stride*i, dim2_stride*j))\n",
    "    return np.asarray(cropped_imgs)\n",
    "\n",
    "\n",
    "# IT IS NOT USED FOR PAD AND CROP DATA OPERATION\n",
    "# IF YOU WANT TO USE CROP AND PAD USE THIS FUNCTION\n",
    "\"\"\"\n",
    "def multi_padding(images, in_size, out_size, mode):\n",
    "    '''Pad the images to in_size\n",
    "    Args :\n",
    "        images : numpy array of images (CxHxW)\n",
    "        in_size(int) : the input_size of model (512)\n",
    "        out_size(int) : the output_size of model (388)\n",
    "        mode(str) : mode of padding\n",
    "    Return :\n",
    "        padded_imgs: numpy arrays of padded images\n",
    "    '''\n",
    "    pad_size = int((in_size - out_size)/2)\n",
    "    padded_imgs = []\n",
    "    for num in range(images.shape[0]):\n",
    "        padded_imgs.append(add_padding(images[num], in_size, out_size, mode=mode))\n",
    "    return np.asarray(padded_imgs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def cropping(image, crop_size, dim1, dim2):\n",
    "    \"\"\"crop the image and pad it to in_size\n",
    "    Args :\n",
    "        images : numpy array of images\n",
    "        crop_size(int) : size of cropped image\n",
    "        dim1(int) : vertical location of crop\n",
    "        dim2(int) : horizontal location of crop\n",
    "    Return :\n",
    "        cropped_img: numpy array of cropped image\n",
    "    \"\"\"\n",
    "    cropped_img = image[dim1:dim1+crop_size, dim2:dim2+crop_size]\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "def add_padding(image, in_size, out_size, mode):\n",
    "    \"\"\"Pad the image to in_size\n",
    "    Args :\n",
    "        images : numpy array of images\n",
    "        in_size(int) : the input_size of model\n",
    "        out_size(int) : the output_size of model\n",
    "        mode(str) : mode of padding\n",
    "    Return :\n",
    "        padded_img: numpy array of padded image\n",
    "    \"\"\"\n",
    "    pad_size = int((in_size - out_size)/2)\n",
    "    padded_img = np.pad(image, pad_size, mode=mode)\n",
    "    return padded_img\n",
    "\n",
    "\n",
    "def division_array(crop_size, crop_num1, crop_num2, dim1, dim2):\n",
    "    \"\"\"Make division array\n",
    "    Args :\n",
    "        crop_size(int) : size of cropped image\n",
    "        crop_num2 (int) : number of crop in horizontal way\n",
    "        crop_num1 (int) : number of crop in vertical way\n",
    "        dim1(int) : vertical size of output\n",
    "        dim2(int) : horizontal size_of_output\n",
    "    Return :\n",
    "        div_array : numpy array of numbers of 1,2,4\n",
    "    \"\"\"\n",
    "    div_array = np.zeros([dim1, dim2])  # make division array\n",
    "    one_array = np.ones([crop_size, crop_size])  # one array to be added to div_array\n",
    "    dim1_stride = stride_size(dim1, crop_num1, crop_size)  # vertical stride\n",
    "    dim2_stride = stride_size(dim2, crop_num2, crop_size)  # horizontal stride\n",
    "    for i in range(crop_num1):\n",
    "        for j in range(crop_num2):\n",
    "            # add ones to div_array at specific position\n",
    "            div_array[dim1_stride*i:dim1_stride*i + crop_size,\n",
    "                      dim2_stride*j:dim2_stride*j + crop_size] += one_array\n",
    "    return div_array\n",
    "\n",
    "\n",
    "def image_concatenate(image, crop_num1, crop_num2, dim1, dim2):\n",
    "    \"\"\"concatenate images\n",
    "    Args :\n",
    "        image : output images (should be square)\n",
    "        crop_num2 (int) : number of crop in horizontal way (2)\n",
    "        crop_num1 (int) : number of crop in vertical way (2)\n",
    "        dim1(int) : vertical size of output (512)\n",
    "        dim2(int) : horizontal size_of_output (512)\n",
    "    Return :\n",
    "        div_array : numpy arrays of numbers of 1,2,4\n",
    "    \"\"\"\n",
    "    crop_size = image.shape[1]  # size of crop\n",
    "    empty_array = np.zeros([dim1, dim2]).astype(\"float64\")  # to make sure no overflow\n",
    "    dim1_stride = stride_size(dim1, crop_num1, crop_size)  # vertical stride\n",
    "    dim2_stride = stride_size(dim2, crop_num2, crop_size)  # horizontal stride\n",
    "    index = 0\n",
    "    for i in range(crop_num1):\n",
    "        for j in range(crop_num2):\n",
    "            # add image to empty_array at specific position\n",
    "            empty_array[dim1_stride*i:dim1_stride*i + crop_size,\n",
    "                        dim2_stride*j:dim2_stride*j + crop_size] += image[index]\n",
    "            index += 1\n",
    "    return empty_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SEMDataTrain(Dataset):\n",
    "\n",
    "    def __init__(self, image_path, mask_path, in_size=572, out_size=388):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_path (str): the path where the image is located\n",
    "            mask_path (str): the path where the mask is located\n",
    "            option (str): decide which dataset to import\n",
    "        \"\"\"\n",
    "        # all file names\n",
    "        self.mask_arr = glob.glob(str(mask_path) + \"/*\")\n",
    "        self.image_arr = glob.glob(str(image_path) + str(\"/*\"))\n",
    "        self.in_size, self.out_size = in_size, out_size\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.mask_arr)\n",
    "        # calculate mean and stdev\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get specific data corresponding to the index\n",
    "        Args:\n",
    "            index (int): index of the data\n",
    "        Returns:\n",
    "            Tensor: specific data on index which is converted to Tensor\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # GET IMAGE\n",
    "        \"\"\"\n",
    "        single_image_name = self.image_arr[index]\n",
    "        img_as_img = Image.open(single_image_name)\n",
    "        # img_as_img.show()\n",
    "        img_as_np = np.asarray(img_as_img)\n",
    "\n",
    "        # Augmentation\n",
    "        # flip {0: vertical, 1: horizontal, 2: both, 3: none}\n",
    "        flip_num = randint(0, 3)\n",
    "        img_as_np = flip(img_as_np, flip_num)\n",
    "\n",
    "        # Noise Determine {0: Gaussian_noise, 1: uniform_noise\n",
    "        if randint(0, 1):\n",
    "            # Gaussian_noise\n",
    "            gaus_sd, gaus_mean = randint(0, 20), 0\n",
    "            img_as_np = add_gaussian_noise(img_as_np, gaus_mean, gaus_sd)\n",
    "        else:\n",
    "            # uniform_noise\n",
    "            l_bound, u_bound = randint(-20, 0), randint(0, 20)\n",
    "            img_as_np = add_uniform_noise(img_as_np, l_bound, u_bound)\n",
    "\n",
    "        # Brightness\n",
    "        pix_add = randint(-20, 20)\n",
    "        img_as_np = change_brightness(img_as_np, pix_add)\n",
    "\n",
    "        # Elastic distort {0: distort, 1:no distort}\n",
    "        sigma = randint(6, 12)\n",
    "        # sigma = 4, alpha = 34\n",
    "        img_as_np, seed = add_elastic_transform(img_as_np, alpha=34, sigma=sigma, pad_size=20)\n",
    "\n",
    "        # Crop the image\n",
    "        img_height, img_width = img_as_np.shape[0], img_as_np.shape[1]\n",
    "        pad_size = int((self.in_size - self.out_size)/2)\n",
    "        img_as_np = np.pad(img_as_np, pad_size, mode=\"symmetric\")\n",
    "        y_loc, x_loc = randint(0, img_height-self.out_size), randint(0, img_width-self.out_size)\n",
    "        img_as_np = cropping(img_as_np, crop_size=self.in_size, dim1=y_loc, dim2=x_loc)\n",
    "        '''\n",
    "        # Sanity Check for image\n",
    "        img1 = Image.fromarray(img_as_np)\n",
    "        img1.show()\n",
    "        '''\n",
    "        # Normalize the image\n",
    "        img_as_np = normalization2(img_as_np, max=1, min=0)\n",
    "        img_as_np = np.expand_dims(img_as_np, axis=0)  # add additional dimension\n",
    "        img_as_tensor = torch.from_numpy(img_as_np).float()  # Convert numpy array to tensor\n",
    "\n",
    "        \"\"\"\n",
    "        # GET MASK\n",
    "        \"\"\"\n",
    "        single_mask_name = self.mask_arr[index]\n",
    "        msk_as_img = Image.open(single_mask_name)\n",
    "        # msk_as_img.show()\n",
    "        msk_as_np = np.asarray(msk_as_img)\n",
    "\n",
    "        # flip the mask with respect to image\n",
    "        msk_as_np = flip(msk_as_np, flip_num)\n",
    "\n",
    "        # elastic_transform of mask with respect to image\n",
    "\n",
    "        # sigma = 4, alpha = 34, seed = from image transformation\n",
    "        msk_as_np, _ = add_elastic_transform(\n",
    "            msk_as_np, alpha=34, sigma=sigma, seed=seed, pad_size=20)\n",
    "        msk_as_np = approximate_image(msk_as_np)  # images only with 0 and 255\n",
    "\n",
    "        # Crop the mask\n",
    "        msk_as_np = cropping(msk_as_np, crop_size=self.out_size, dim1=y_loc, dim2=x_loc)\n",
    "        '''\n",
    "        # Sanity Check for mask\n",
    "        img2 = Image.fromarray(msk_as_np)\n",
    "        img2.show()\n",
    "        '''\n",
    "\n",
    "        # Normalize mask to only 0 and 1\n",
    "        msk_as_np = msk_as_np/255\n",
    "        # msk_as_np = np.expand_dims(msk_as_np, axis=0)  # add additional dimension\n",
    "        msk_as_tensor = torch.from_numpy(msk_as_np).long()  # Convert numpy array to tensor\n",
    "\n",
    "        return (img_as_tensor, msk_as_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            length (int): length of the data\n",
    "        \"\"\"\n",
    "        return self.data_len\n",
    "\n",
    "\n",
    "class SEMDataVal(Dataset):\n",
    "    def __init__(self, image_path, mask_path, in_size=572, out_size=388):\n",
    "        '''\n",
    "        Args:\n",
    "            image_path = path where test images are located\n",
    "            mask_path = path where test masks are located\n",
    "        '''\n",
    "        # paths to all images and masks\n",
    "        self.mask_arr = glob.glob(str(mask_path) + str(\"/*\"))\n",
    "        self.image_arr = glob.glob(str(image_path) + str(\"/*\"))\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.data_len = len(self.mask_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get specific data corresponding to the index\n",
    "        Args:\n",
    "            index : an integer variable that calls (indext)th image in the\n",
    "                    path\n",
    "        Returns:\n",
    "            Tensor: 4 cropped data on index which is converted to Tensor\n",
    "        \"\"\"\n",
    "        single_image = self.image_arr[index]\n",
    "        img_as_img = Image.open(single_image)\n",
    "        # img_as_img.show()\n",
    "        # Convert the image into numpy array\n",
    "        img_as_np = np.asarray(img_as_img)\n",
    "\n",
    "        # Make 4 cropped image (in numpy array form) using values calculated above\n",
    "        # Cropped images will also have paddings to fit the model.\n",
    "        pad_size = int((self.in_size - self.out_size)/2)\n",
    "        img_as_np = np.pad(img_as_np, pad_size, mode=\"symmetric\")\n",
    "        img_as_np = multi_cropping(img_as_np,\n",
    "                                   crop_size=self.in_size,\n",
    "                                   crop_num1=2, crop_num2=2)\n",
    "\n",
    "        # Empty list that will be filled in with arrays converted to tensor\n",
    "        processed_list = []\n",
    "\n",
    "        for array in img_as_np:\n",
    "\n",
    "            # SANITY CHECK: SEE THE CROPPED & PADDED IMAGES\n",
    "            #array_image = Image.fromarray(array)\n",
    "\n",
    "            # Normalize the cropped arrays\n",
    "            img_to_add = normalization2(array, max=1, min=0)\n",
    "            # Convert normalized array into tensor\n",
    "            processed_list.append(img_to_add)\n",
    "\n",
    "        img_as_tensor = torch.Tensor(processed_list)\n",
    "        #  return tensor of 4 cropped images\n",
    "        #  top left, top right, bottom left, bottom right respectively.\n",
    "\n",
    "        \"\"\"\n",
    "        # GET MASK\n",
    "        \"\"\"\n",
    "        single_mask_name = self.mask_arr[index]\n",
    "        msk_as_img = Image.open(single_mask_name)\n",
    "        # msk_as_img.show()\n",
    "        msk_as_np = np.asarray(msk_as_img)\n",
    "        # Normalize mask to only 0 and 1\n",
    "        msk_as_np = multi_cropping(msk_as_np,\n",
    "                                   crop_size=self.out_size,\n",
    "                                   crop_num1=2, crop_num2=2)\n",
    "\n",
    "        msk_as_np = msk_as_np/255\n",
    "\n",
    "        # msk_as_np = np.expand_dims(msk_as_np, axis=0)  # add additional dimension\n",
    "        msk_as_tensor = torch.from_numpy(msk_as_np).long()  # Convert numpy array to tensor\n",
    "        original_msk = torch.from_numpy(np.asarray(msk_as_img))\n",
    "        return (img_as_tensor, msk_as_tensor, original_msk)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.data_len\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train_dir = 'data/images/train/'\n",
    "image_valid_dir = 'data/images/valid/'\n",
    "mask_train_dir ='data/mask/train/'\n",
    "mask_valid_dir = 'data/mask/valid/'\n",
    "\n",
    "train_data = SEMDataTrain(image_train_dir,mask_train_dir, in_size=572, out_size=388 )\n",
    "\n",
    "valid_data = SEMDataVal(image_valid_dir,mask_valid_dir, in_size=572, out_size=388 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                             batch_size=16, shuffle=False,\n",
    "                                             num_workers=12)\n",
    "valid_image_loader = torch.utils.data.DataLoader(valid_data,\n",
    "                                             batch_size=16, shuffle=False,\n",
    "                                             num_workers=12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
    "\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "class Double_conv(nn.Module):\n",
    "\n",
    "    '''(conv => ReLU) * 2 => MaxPool2d'''\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_ch(int) : input channel\n",
    "            out_ch(int) : output channel\n",
    "        \"\"\"\n",
    "        super(Double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=0, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=0, stride=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv_down(nn.Module):\n",
    "\n",
    "    '''(conv => ReLU) * 2 => MaxPool2d'''\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_ch(int) : input channel\n",
    "            out_ch(int) : output channel\n",
    "        \"\"\"\n",
    "        super(Conv_down, self).__init__()\n",
    "        self.conv = Double_conv(in_ch, out_ch)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        pool_x = self.pool(x)\n",
    "        return pool_x, x\n",
    "\n",
    "\n",
    "class Conv_up(nn.Module):\n",
    "\n",
    "    '''(conv => ReLU) * 2 => MaxPool2d'''\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_ch(int) : input channel\n",
    "            out_ch(int) : output channel\n",
    "        \"\"\"\n",
    "        super(Conv_up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = Double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x1_dim = x1.size()[2]\n",
    "        x2 = extract_img(x1_dim, x2)\n",
    "        x1 = torch.cat((x1, x2), dim=1)\n",
    "        x1 = self.conv(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "def extract_img(size, in_tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        size(int) : size of cut\n",
    "        in_tensor(tensor) : tensor to be cut\n",
    "    \"\"\"\n",
    "    dim1, dim2 = in_tensor.size()[2:]\n",
    "    in_tensor = in_tensor[:, :, int((dim1-size)/2):int((dim1+size)/2),\n",
    "                          int((dim2-size)/2):int((dim2+size)/2)]\n",
    "    return in_tensor\n",
    "\n",
    "\n",
    "class CleanU_Net(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CleanU_Net, self).__init__()\n",
    "        self.Conv_down1 = Conv_down(in_channels, 64)\n",
    "        self.Conv_down2 = Conv_down(64, 128)\n",
    "        self.Conv_down3 = Conv_down(128, 256)\n",
    "        self.Conv_down4 = Conv_down(256, 512)\n",
    "        self.Conv_down5 = Conv_down(512, 1024)\n",
    "        self.Conv_up1 = Conv_up(1024, 512)\n",
    "        self.Conv_up2 = Conv_up(512, 256)\n",
    "        self.Conv_up3 = Conv_up(256, 128)\n",
    "        self.Conv_up4 = Conv_up(128, 64)\n",
    "        self.Conv_out = nn.Conv2d(64, out_channels, 1, padding=0, stride=1)\n",
    "        #self.Conv_final = nn.Conv2d(out_channels, out_channels, 1, padding=0, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x, conv1 = self.Conv_down1(x)\n",
    "        #print(\"dConv1 => down1|\", x.shape)\n",
    "        x, conv2 = self.Conv_down2(x)\n",
    "        #print(\"dConv2 => down2|\", x.shape)\n",
    "        x, conv3 = self.Conv_down3(x)\n",
    "        #print(\"dConv3 => down3|\", x.shape)\n",
    "        x, conv4 = self.Conv_down4(x)\n",
    "        #print(\"dConv4 => down4|\", x.shape)\n",
    "        _, x = self.Conv_down5(x)\n",
    "        #print(\"dConv5|\", x.shape)\n",
    "        x = self.Conv_up1(x, conv4)\n",
    "        #print(\"up1 => uConv1|\", x.shape)\n",
    "        x = self.Conv_up2(x, conv3)\n",
    "        #print(\"up2 => uConv2|\", x.shape)\n",
    "        x = self.Conv_up3(x, conv2)\n",
    "        #print(\"up3 => uConv3|\", x.shape)\n",
    "        x = self.Conv_up4(x, conv1)\n",
    "        x = self.Conv_out(x)\n",
    "        #x = self.Conv_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "model = CleanU_Net(1, 2).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (616,807) (616,807,43) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-e3ed0caaba5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m \u001b[0mmodel_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-e3ed0caaba5f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_train, criterion, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-1ed44e1cd93f>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# sigma = 4, alpha = 34\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mimg_as_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_elastic_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_as_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Crop the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-fe10b7b8b2d9>\u001b[0m in \u001b[0;36madd_elastic_transform\u001b[1;34m(image, alpha, sigma, pad_size, seed)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcropping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (616,807) (616,807,43) "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model(model, data_train, criterion, optimizer):\n",
    "    \"\"\"Train the model and report validation error with training error\n",
    "    Args:\n",
    "        model: the model to be trained\n",
    "        criterion: loss function\n",
    "        data_train (DataLoader): training dataset\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch, (images, masks) in enumerate(data_train):\n",
    "        images = Variable(images.cuda())\n",
    "        masks = Variable(masks.cuda())\n",
    "        outputs = model(images)\n",
    "        # print(masks.shape, outputs.shape)\n",
    "        loss = criterion(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    # total_loss = get_loss_train(model, data_train, criterion)\n",
    "\n",
    "    \n",
    "def get_loss_train(model, data_train, criterion):\n",
    "    \"\"\"\n",
    "        Calculate loss over train set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for batch, (images, masks) in enumerate(data_train):\n",
    "        with torch.no_grad():\n",
    "            images = Variable(images.cuda())\n",
    "            masks = Variable(masks.cuda())\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            preds = torch.argmax(outputs, dim=1).float()\n",
    "            acc = accuracy_check_for_batch(masks.cpu(), preds.cpu(), images.size()[0])\n",
    "            total_acc = total_acc + acc\n",
    "            total_loss = total_loss + loss.cpu().item()\n",
    "    return total_acc/(batch+1), total_loss/(batch + 1)\n",
    "\n",
    "\n",
    "def validate_model(model, data_val, criterion, epoch, make_prediction=True, save_folder_name='prediction'):\n",
    "    \"\"\"\n",
    "        Validation run\n",
    "    \"\"\"\n",
    "    # calculating validation loss\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    for batch, (images_v, masks_v, original_msk) in enumerate(data_val):\n",
    "        stacked_img = torch.Tensor([]).cuda()\n",
    "        for index in range(images_v.size()[1]):\n",
    "            with torch.no_grad():\n",
    "                image_v = Variable(images_v[:, index, :, :].unsqueeze(0).cuda())\n",
    "                mask_v = Variable(masks_v[:, index, :, :].squeeze(1).cuda())\n",
    "                # print(image_v.shape, mask_v.shape)\n",
    "                output_v = model(image_v)\n",
    "                total_val_loss = total_val_loss + criterion(output_v, mask_v).cpu().item()\n",
    "                # print('out', output_v.shape)\n",
    "                output_v = torch.argmax(output_v, dim=1).float()\n",
    "                stacked_img = torch.cat((stacked_img, output_v))\n",
    "        if make_prediction:\n",
    "            im_name = batch  # TODO: Change this to real image name so we know\n",
    "            pred_msk = save_prediction_image(stacked_img, im_name, epoch, save_folder_name)\n",
    "            acc_val = accuracy_check(original_msk, pred_msk)\n",
    "            total_val_acc = total_val_acc + acc_val\n",
    "\n",
    "    return total_val_acc/(batch + 1), total_val_loss/((batch + 1)*4)    \n",
    "\n",
    "def save_prediction_image(stacked_img, im_name, epoch, save_folder_name=\"result_images\", save_im=True):\n",
    "    \"\"\"save images to save_path\n",
    "    Args:\n",
    "        stacked_img (numpy): stacked cropped images\n",
    "        save_folder_name (str): saving folder name\n",
    "    \"\"\"\n",
    "    div_arr = division_array(388, 2, 2, 512, 512)\n",
    "    img_cont = image_concatenate(stacked_img.cpu().data.numpy(), 2, 2, 512, 512)\n",
    "    img_cont = polarize((img_cont)/div_arr)*255\n",
    "    img_cont_np = img_cont.astype('uint8')\n",
    "    img_cont = Image.fromarray(img_cont_np)\n",
    "    # organize images in every epoch\n",
    "    desired_path = save_folder_name + '/epoch_' + str(epoch) + '/'\n",
    "    # Create the path if it does not exist\n",
    "    if not os.path.exists(desired_path):\n",
    "        os.makedirs(desired_path)\n",
    "    # Save Image!\n",
    "    export_name = str(im_name) + '.png'\n",
    "    img_cont.save(desired_path + export_name)\n",
    "    return img_cont_np\n",
    "def polarize(img):\n",
    "    ''' Polarize the value to zero and one\n",
    "    Args:\n",
    "        img (numpy): numpy array of image to be polarized\n",
    "    return:\n",
    "        img (numpy): numpy array only with zero and one\n",
    "    '''\n",
    "    img[img >= 0.5] = 1\n",
    "    img[img < 0.5] = 0\n",
    "    return img\n",
    "    \n",
    "model_ft = train_model(model, train_data, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-89-024321e806b3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-89-024321e806b3>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for i in range(epoch_start, epoch_end):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Training!\")\n",
    "    for i in range(epoch_start, epoch_end):\n",
    "        # train the model\n",
    "        train_model(model, SEM_train_load, criterion, optimizer)\n",
    "        train_acc, train_loss = get_loss_train(model, SEM_train_load, criterion)\n",
    "\n",
    "        #train_loss = train_loss / len(SEM_train)\n",
    "        print('Epoch', str(i+1), 'Train loss:', train_loss, \"Train acc\", train_acc)\n",
    "\n",
    "        # Validation every 5 epoch\n",
    "        if (i+1) % 5 == 0:\n",
    "            val_acc, val_loss = validate_model(\n",
    "                model, SEM_val_load, criterion, i+1, True, image_save_path)\n",
    "            print('Val loss:', val_loss, \"val acc:\", val_acc)\n",
    "            values = [i+1, train_loss, train_acc, val_loss, val_acc]\n",
    "            export_history(header, values, save_dir, save_file_name)\n",
    "\n",
    "            if (i+1) % 100 == 0:  # save model every 10 epoch\n",
    "                save_models(model, model_save_dir, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
